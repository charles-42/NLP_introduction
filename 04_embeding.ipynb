{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Flatten,Embedding,Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I Introduction au problème"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'inspire de ce [tuto](https://medium.com/analytics-vidhya/understanding-embedding-layer-in-keras-bbe3ff1327ce#:~:text=Embedding%20layer%20enables%20us%20to,way%20along%20with%20reduced%20dimensions)\n",
    "L'idée est de classifier des avis en fonction qu'ils sont positifs (1) ou  négatif (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 10 restaurant reviews\n",
    "reviews =[\n",
    "          'Never coming back!, Never!',\n",
    "          'horrible service',\n",
    "          'rude waitress',\n",
    "          'cold food',\n",
    "          'horrible food!',\n",
    "          'awesome, food is okay, atmosphere great',\n",
    "          'awesome services!, I love this place',\n",
    "          'rocks',\n",
    "          'poor work, my eyes crie',\n",
    "          'couldn\\'t have done better, waitress is super nice'\n",
    "]\n",
    "#Define labels\n",
    "labels = array([0,0,0,0,0,1,1,1,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainer son propre embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded reviews:\n",
      "[3, 20, 22, 3]\n",
      "[28, 36]\n",
      "[20, 26]\n",
      "[42, 13]\n",
      "[28, 13]\n",
      "[27, 13, 1, 9, 44, 2]\n",
      "[27, 24, 48, 3, 23, 17]\n",
      "[26]\n",
      "[9, 15, 9, 13, 14]\n",
      "[10, 15, 15, 42, 26, 1, 17, 13]\n"
     ]
    }
   ],
   "source": [
    "Vocab_size = 50\n",
    "encoded_reviews = [one_hot(d,Vocab_size) for d in reviews]\n",
    "print(f'encoded reviews:')\n",
    "for t in encoded_reviews:\n",
    "    print (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paded reviews:\n",
      "[ 3 20 22  3  0]\n",
      "[28 36  0  0  0]\n",
      "[20 26  0  0  0]\n",
      "[42 13  0  0  0]\n",
      "[28 13  0  0  0]\n",
      "[13  1  9 44  2]\n",
      "[24 48  3 23 17]\n",
      "[26  0  0  0  0]\n",
      "[ 9 15  9 13 14]\n",
      "[42 26  1 17 13]\n"
     ]
    }
   ],
   "source": [
    "max_length = 5\n",
    "padded_reviews = pad_sequences(encoded_reviews,maxlen=max_length,padding='post')\n",
    "print(f'paded reviews:')\n",
    "for t in padded_reviews:\n",
    "    print (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 10:51:18.716190: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 5, 8)              400       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 40)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 441\n",
      "Trainable params: 441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(input_dim=Vocab_size,output_dim=8,input_length=max_length)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14240f040>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_reviews,labels,epochs=100,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 8)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.get_weights()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03984213 -0.06080137  0.13901348  0.06346074 -0.05954554  0.02771615\n",
      "  0.14004043 -0.09462351]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.get_weights()[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III Utiliser un embedding pré-entrainé (Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "df_train = pd.DataFrame(data= np.c_[twenty_train['data'], twenty_train['target']], columns= ['text','target'])\n",
    "df_test = pd.DataFrame(data= np.c_[twenty_test['data'], twenty_test['target']], columns= ['text','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/engine/base_preprocessing_layer.py\", line 118, in adapt_step  *\n        self.update_state(data)\n    File \"/Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 431, in update_state  **\n        self._lookup_layer.update_state(self._preprocess(data))\n    File \"/Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 520, in _preprocess\n        raise ValueError(\n\n    ValueError: When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, 2) with rank=2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/charles/Documents/pythonProject/NLP_introduction/04_embeding.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/charles/Documents/pythonProject/NLP_introduction/04_embeding.ipynb#ch0000016?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/charles/Documents/pythonProject/NLP_introduction/04_embeding.ipynb#ch0000016?line=3'>4</a>\u001b[0m vectorizer \u001b[39m=\u001b[39m TextVectorization(max_tokens\u001b[39m=\u001b[39m\u001b[39m20000\u001b[39m, output_sequence_length\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/charles/Documents/pythonProject/NLP_introduction/04_embeding.ipynb#ch0000016?line=4'>5</a>\u001b[0m vectorizer\u001b[39m.\u001b[39;49madapt(df_train)\n",
      "File \u001b[0;32m~/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py:428\u001b[0m, in \u001b[0;36mTextVectorization.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py?line=380'>381</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madapt\u001b[39m(\u001b[39mself\u001b[39m, data, batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, steps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py?line=381'>382</a>\u001b[0m   \u001b[39m\"\"\"Computes a vocabulary of string terms from tokens in a dataset.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py?line=382'>383</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py?line=383'>384</a>\u001b[0m \u001b[39m  Calling `adapt()` on a `TextVectorization` layer is an alternative to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py?line=425'>426</a>\u001b[0m \u001b[39m        argument is not supported with array inputs.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py?line=426'>427</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py?line=427'>428</a>\u001b[0m   \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49madapt(data, batch_size\u001b[39m=\u001b[39;49mbatch_size, steps\u001b[39m=\u001b[39;49msteps)\n",
      "File \u001b[0;32m~/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/engine/base_preprocessing_layer.py:249\u001b[0m, in \u001b[0;36mPreprocessingLayer.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/engine/base_preprocessing_layer.py?line=246'>247</a>\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/engine/base_preprocessing_layer.py?line=247'>248</a>\u001b[0m   \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m--> <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/engine/base_preprocessing_layer.py?line=248'>249</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_adapt_function(iterator)\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/engine/base_preprocessing_layer.py?line=249'>250</a>\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/engine/base_preprocessing_layer.py?line=250'>251</a>\u001b[0m       context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=152'>153</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=153'>154</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=154'>155</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1148'>1149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/engine/base_preprocessing_layer.py\", line 118, in adapt_step  *\n        self.update_state(data)\n    File \"/Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 431, in update_state  **\n        self._lookup_layer.update_state(self._preprocess(data))\n    File \"/Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 520, in _preprocess\n        raise ValueError(\n\n    ValueError: When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, 2) with rank=2\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow as tf\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "vectorizer.adapt(df_train)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6e043ccc4ff5f6895d6ef4ef51b514d0f7ca0307d37f45ef7621519e85018ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
