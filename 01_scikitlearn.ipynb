{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce tutoriel reprend celui sur le site de [scikit learn](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I Analyse du problème"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utilisé un dataset inclu dans scikit learn.\n",
    "\n",
    "Il contient une série de texte classé par catégorie. Nous allons nous ateler à une tâche de classification supervisée dont le but sera de retrouver cette catégorie.\n",
    "\n",
    "Nous commençons par selectionner un sous ensemble de ces catégories pour que le dataset ne soit pas trop important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe ensuite le jeu de données correspondant à ces catégories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut afficher les premières lignes d'un des fichiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: Mike_Peredo@mindlink.bc.ca (Mike Peredo)\n",
      "Subject: Re: \"Fake\" virtual reality\n",
      "Organization: MIND LINK! - British Columbia, Canada\n",
      "Lines: 11\n",
      "\n",
      "The most ridiculous example of VR-exploitation I've seen so far is the\n",
      "\"Virtual Reality Clothing Company\" which recently opened up in Vancouver. As\n",
      "far as I can tell it's just another \"chic\" clothes spot. Although it would be\n",
      "interesting if they were selling \"virtual clothing\"....\n",
      "\n",
      "E-mail me if you want me to dig up their phone # and you can probably get\n",
      "some promotional lit.\n",
      "\n",
      "MP\n",
      "(8^)-\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(twenty_train.data[15].split(\"\\n\")[:15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II Choix de l'encodage et du pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Théorie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn implémente différent encodage ([doc](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text)):\n",
    "- CountVectorizer: Transforme une série de documents en une matrice de comptage (Bag of Word)\n",
    "- HashingVectorizer: Transforme une série de documents en une matrice d'occurence\n",
    "- TfidfVectorizer: Transforme une série de documents en une matrice de fréquence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous appliquerons dans un premier temps CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans scikit learn, le pre-processing est réalisé via les paramètres de la classe d'encodage.\n",
    "On a en effet les paramètres suivants (entre parenthèse la valeur par défaut):\n",
    "- strip_accents (FALSE): supprime tous les accents (réduit la taille du vocabulaire)\n",
    "- lowercase (TRUE): convertie tous les caractères (réduit la taille du vocabulaire)\n",
    "- stop_words (None): permet d'ajouter une liste de stop word ou d'utiliser une liste par défaut {english}\n",
    "- max_df (1.0): ignore les mots qui ont une fréquence trop élevée dans le dataset (autre manière de gérer les stopwords)\n",
    "- min_df (1): ignore les mots présents moins de fois que le seuil. Permet de réduire la taille de la matrice et également l'overfitting.\n",
    "- analyzer (word) : Qu'est ce qui constitue une token (une unité de texte) {‘word’, ‘char’, ‘char_wb’} or callable (callable permet d'utiliser un preprocessing externe)\n",
    "- ngram_range (1,1): taille du n-gram à analyser\n",
    "- vocabulary: permet de proposer un vocabulaire précis\n",
    "\n",
    "Nous n'avons pas toutes les options de pre processing comme ce qui est inclu dans les packages comme nltk ou spacy mais nous verons plus tard qu'il sera possible de les utilser dans scikit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Pratique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous créons donc la matrice de comptage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de documents dans le jeu d'entrainement: 2257\n",
      "dimension de la matrice de comptage: (2257, 18487)\n"
     ]
    }
   ],
   "source": [
    "# Dans notre jeu d'entrainement il y a 2257 documents\n",
    "print(f\"nombre de documents dans le jeu d'entrainement: {len(twenty_train.data)}\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(max_df=0.9, min_df=2)\n",
    "# On commence avec un preprocessing minimal (lower, token = word)\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "print(f\"dimension de la matrice de comptage: {X_train_counts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3979)\t4\n",
      "  (0, 1440)\t2\n",
      "  (0, 17207)\t2\n",
      "  (0, 10924)\t3\n",
      "  (0, 4169)\t3\n",
      "  (0, 4651)\t2\n",
      "  (0, 8714)\t1\n",
      "  (0, 8507)\t2\n",
      "  (0, 9947)\t2\n",
      "  (0, 8678)\t2\n",
      "  (0, 11700)\t1\n",
      "  (0, 13012)\t1\n",
      "  (0, 8486)\t1\n",
      "  (0, 8063)\t1\n",
      "  (0, 17374)\t2\n",
      "  (0, 202)\t1\n",
      "  (0, 5892)\t1\n",
      "  (0, 2104)\t1\n",
      "  (0, 9824)\t1\n",
      "  (0, 7810)\t1\n",
      "  (0, 17970)\t1\n",
      "  (0, 15788)\t1\n",
      "  (0, 12520)\t1\n",
      "  (0, 2162)\t1\n",
      "  (0, 12525)\t1\n",
      "  :\t:\n",
      "  (0, 10163)\t1\n",
      "  (0, 5869)\t1\n",
      "  (0, 14688)\t1\n",
      "  (0, 8511)\t1\n",
      "  (0, 12879)\t1\n",
      "  (0, 12865)\t1\n",
      "  (0, 6317)\t2\n",
      "  (0, 2100)\t1\n",
      "  (0, 14273)\t1\n",
      "  (0, 9356)\t1\n",
      "  (0, 16676)\t1\n",
      "  (0, 4727)\t1\n",
      "  (0, 7949)\t1\n",
      "  (0, 16607)\t1\n",
      "  (0, 8819)\t1\n",
      "  (0, 1653)\t1\n",
      "  (0, 13323)\t1\n",
      "  (0, 4349)\t1\n",
      "  (0, 17360)\t1\n",
      "  (0, 16494)\t1\n",
      "  (0, 52)\t2\n",
      "  (0, 861)\t2\n",
      "  (0, 1125)\t1\n",
      "  (0, 10308)\t1\n",
      "  (0, 7050)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_counts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après la dimension de la matrice de comptage on déduit qu'il y a 35788 mots dans notre vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1815\n",
      "4349\n",
      "7810\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# on peut retrouver l'indexe de chaque mot\n",
    "print(count_vect.vocabulary_.get(u'algorithm'))\n",
    "print(count_vect.vocabulary_.get(u'computer'))\n",
    "print(count_vect.vocabulary_.get(u'good'))\n",
    "print(count_vect.vocabulary_.get(u'the'))\n",
    "\n",
    "# plus un mot est fréquent plus sont indexe est elevé "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III Entainement et prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il ne reste plus qu'à entrainer un modèle qui prendra pour features la matrice construite et pour target le label de chacun des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_counts, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_counts = count_vect.transform(twenty_test.data)\n",
    "predicted = clf.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9394141145139814"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(twenty_test.target, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.92      0.92      0.92       319\n",
      "         comp.graphics       0.93      0.96      0.95       389\n",
      "               sci.med       0.96      0.91      0.94       396\n",
      "soc.religion.christian       0.94      0.96      0.95       398\n",
      "\n",
      "              accuracy                           0.94      1502\n",
      "             macro avg       0.94      0.94      0.94      1502\n",
      "          weighted avg       0.94      0.94      0.94      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n",
      "'a scalpel is a medical tool' => sci.med\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast', 'a scalpel is a medical tool']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "predicted = clf.predict(X_new_counts)\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV Tuning du pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée pour cela un Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'vect__strip_accents': [None, 'ascii'],\n",
    "    'vect__stop_words': [\"english\", None],\n",
    "    'vect__max_df': [1, 0.9,0.8,0.7],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)\n",
    "# on ne l'entraine que sur une partie du jeu de données\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9778427486607931\n",
      "vect__max_df: 0.9\n",
      "vect__ngram_range: (1, 1)\n",
      "vect__stop_words: 'english'\n",
      "vect__strip_accents: None\n"
     ]
    }
   ],
   "source": [
    "print(gs_clf.best_score_)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_grid = gs_clf.predict(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.93      0.91      0.92       319\n",
      "         comp.graphics       0.95      0.96      0.96       389\n",
      "               sci.med       0.95      0.93      0.94       396\n",
      "soc.religion.christian       0.93      0.96      0.95       398\n",
      "\n",
      "              accuracy                           0.94      1502\n",
      "             macro avg       0.94      0.94      0.94      1502\n",
      "          weighted avg       0.94      0.94      0.94      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(twenty_test.target, predicted_grid, target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V Preprocessing avec NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/charles/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "vect_nltk = CountVectorizer(tokenizer=LemmaTokenizer()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "vect_nltk = CountVectorizer(tokenizer=LemmaTokenizer()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_nltk = Pipeline([\n",
    "    ('vect', vect_nltk),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/charles/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/charles/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/charles/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'WordNetLemmatizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/charles/Documents/pythonProject/NLP_introduction/01_scikitlearn.ipynb Cell 40'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/charles/Documents/pythonProject/NLP_introduction/01_scikitlearn.ipynb#ch0000037?line=1'>2</a>\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mwordnet\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/charles/Documents/pythonProject/NLP_introduction/01_scikitlearn.ipynb#ch0000037?line=2'>3</a>\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39momw-1.4\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/charles/Documents/pythonProject/NLP_introduction/01_scikitlearn.ipynb#ch0000037?line=3'>4</a>\u001b[0m gs_clf_nltk \u001b[39m=\u001b[39m text_clf_nltk\u001b[39m.\u001b[39;49mfit(twenty_train\u001b[39m.\u001b[39;49mdata, twenty_train\u001b[39m.\u001b[39;49mtarget)\n",
      "File \u001b[0;32m~/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py:390\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=363'>364</a>\u001b[0m \u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=364'>365</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=365'>366</a>\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=386'>387</a>\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=387'>388</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=388'>389</a>\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m--> <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=389'>390</a>\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=390'>391</a>\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=391'>392</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py:348\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=345'>346</a>\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=346'>347</a>\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=347'>348</a>\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=348'>349</a>\u001b[0m     cloned_transformer,\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=349'>350</a>\u001b[0m     X,\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=350'>351</a>\u001b[0m     y,\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=351'>352</a>\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=352'>353</a>\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=353'>354</a>\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=354'>355</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=355'>356</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=356'>357</a>\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=357'>358</a>\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=358'>359</a>\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=359'>360</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/joblib/memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/joblib/memory.py?line=347'>348</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/joblib/memory.py?line=348'>349</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=890'>891</a>\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=891'>892</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=892'>893</a>\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=893'>894</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/pipeline.py?line=894'>895</a>\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1321'>1322</a>\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1322'>1323</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1323'>1324</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1324'>1325</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1325'>1326</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1326'>1327</a>\u001b[0m             )\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1327'>1328</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1329'>1330</a>\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1331'>1332</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1332'>1333</a>\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1198'>1199</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1199'>1200</a>\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1200'>1201</a>\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1201'>1202</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1202'>1203</a>\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:115\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=112'>113</a>\u001b[0m     doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=114'>115</a>\u001b[0m     doc \u001b[39m=\u001b[39m tokenizer(doc)\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=115'>116</a>\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/charles/Documents/pythonProject/NLP_introduction/env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=116'>117</a>\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'WordNetLemmatizer' object is not callable"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "gs_clf_nltk = text_clf_nltk.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_nltk = gs_clf_nltk.predict(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.90      0.88      0.89       319\n",
      "         comp.graphics       0.92      0.94      0.93       389\n",
      "               sci.med       0.93      0.89      0.91       396\n",
      "soc.religion.christian       0.92      0.95      0.94       398\n",
      "\n",
      "              accuracy                           0.92      1502\n",
      "             macro avg       0.92      0.91      0.91      1502\n",
      "          weighted avg       0.92      0.92      0.92      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(twenty_test.target, predicted_nltk, target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici les résultats sont moins bon mais nous n'avons pas tuné les modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons découvrir tous les modèles suivant au travers de l'exemple d'une compétition kaggle.\n",
    "L'objectif de cette [compétition](https://www.kaggle.com/competitions/quora-insincere-questions-classification/data?select=train.csv) est de déterminer si des questions postées sur le site Quora sont sincères ou non."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consignes:\n",
    "- Commencez par lire les consignes de la compétition pour lire les enjeux, repérer notament la métrique d'interet\n",
    "- Télécharger seulement le train set, nous travaillerons dessus.\n",
    "- Mettez en place un MLflow avec un projet spécifique. Pour tous les modèles que vous lancerez faites remonter spécifiquement le F1 score et des tags correspondant au modèle (quel préprocessing (steaming, url hangling), quel encodage (sous forme de tag) (tf, tfidf, count vect, Word2Vect...), quel modèle (regresison, réseau de neurones)). Si vous lancez plusieurs modèles d'une meme catégorie pensez également à remonter les hyperparamètres modifiés. \n",
    "- Sur le jeu de données, lancez en vous aidant de scikit learn:\n",
    "    - un count vectoriser\n",
    "    - un tf\n",
    "    - un tf idf\n",
    " Vous avez le choix du modèle, vous pouvez meme si vous avez le temps faire un pycaret entre votre base encodée et votre target et tuner les hyperparamètres.\n",
    " - Intégrez ensuite si vous avez le temps du preprocessing avec NLTK.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6e043ccc4ff5f6895d6ef4ef51b514d0f7ca0307d37f45ef7621519e85018ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
